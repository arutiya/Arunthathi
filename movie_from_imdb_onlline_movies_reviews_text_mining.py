# -*- coding: utf-8 -*-
"""MOVIE FROM  IMDB ONLLINE  MOVIES reviews - Text Mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JMVg9-y5tW2VgA0XUn7NimFsRgEtIOsh

# TEXT MINING
In machine learning, classification is used to classify a new observation into a specific set/category based on a training set of data containing observations whose category is known in advance. The most common example is “spam” or “non-spam” classes for emails. In E-Commerce, classifier algorithms can be used to classify sentiments of review based on words. The specific words in the language are categorized in advance for their positive or negative sentiments.
Classification is an instance of supervised learning. Training set has correctly identified observations. Classifier algorithms are used to create cluster/sets from the uncategorized unsupervised data based on similarity and/or distance from the training data set.

**Text Mining is the process of deriving meaningful information from natural language text.**

![image.png](attachment:image.png)

# Text Analytics and NLP
Text communication is one of the most popular forms of day to day conversion. We chat, message, tweet, share status, email, write blogs, share opinion and feedback in our daily routine. All of these activities are generating text in a significant amount, which is unstructured in nature. I this area of the online marketplace and social media, It is essential to analyze vast quantities of data, to understand peoples opinion.

NLP enables the computer to interact with humans in a natural manner. It helps the computer to understand the human language and derive meaning from it. NLP is applicable in several problematic from speech recognition, language translation, classifying documents to information extraction. Analyzing movie review is one of the classic examples to demonstrate a simple NLP Bag-of-words model, on movie reviews.

# Compare Text Analytics, NLP and Text Mining
Text mining also referred to as text analytics. Text mining is a process of exploring sizeable textual data and find patterns. Text Mining process the text itself, while NLP process with the underlying metadata. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining. Natural language processing is one of the components of text mining. NLP helps identified sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithm used to classify information.

# Text Analysis Operations using NLTK
NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. It is free, opensource, easy to use, large community, and well documented. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps the computer to analysis, preprocess, and understand the written text.

# What is NLP?

Natural Language Processing(NLP) is a part of computer science and artificial intelligence which deals with human languages.
In other words, NLP is a component of text mining that performs a special kind of linguistic analysis that essentially helps a machine “read” text. It uses a different methodology to decipher the ambiguities in human language, including the following: automatic summarization, part-of-speech tagging, disambiguation, chunking, as well as disambiguation and natural language understanding and recognition. We will see all the processes in a step by step manner using Python.
![image.png](attachment:image.png)

First, we need to install the NLTK library that is the natural language toolkit for building Python programs to work with human language data and it also provides easy to use interface.

# Sentiment Analysis
Nowadays companies want to understand, what went wrong with their latest products? What users and the general public think about the latest feature? You can quantify such information with reasonable accuracy using sentiment analysis.

Quantifying users content, idea, belief, and opinion is known as sentiment analysis. User's online post, blogs, tweets, feedback of product helps business people to the target audience and innovate in products and services. Sentiment analysis helps in understanding people in a better and more accurate way. It is not only limited to marketing, but it can also be utilized in politics, research, and security.

Human communication just not limited to words, it is more than words. Sentiments are combination words, tone, and writing style. As a data analyst, It is more important to understand our sentiments, what it really means?
![image.png](attachment:image.png)

There are mainly two approaches for performing sentiment analysis.

- Lexicon-based: count number of positive and negative words in given text and the larger count will be the sentiment of text.

- Machine learning based approach: Develop a classification model, which is trained using the pre-labeled dataset of positive, negative, and neutral.

## OBJECTIVE:- Extract reviews of A MOVIE FROM  IMDB ONLLINE  MOVIES website AND Perform sentimental analysis

# IMPORT LIBRARY
"""

import requests 
from textblob import TextBlob# Importing requests to extract content from a url
from bs4 import BeautifulSoup as bs # Beautifulsoup is for web scrapping...used to scrap specific content 
import re 
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
import nltk.corpus
from nltk.corpus import stopwords

"""# EXTRACT REVIEW DATA FROM IMDB WEBSTE"""

#Identifying the URL structure
from requests import get
movie_reviews=[]
url = "https://www.imdb.com/title/tt1187043/reviews?ref_=tt_urv"
response = get(url)
print(response.text)

#Using BeautifulSoup to parse the HTML content
from bs4 import BeautifulSoup
html_soup = BeautifulSoup(response.text, 'html.parser')
type(html_soup)

html_soup.find_all('div', class_='text show-more__control')

results=html_soup.find_all('div', class_='text show-more__control')

results

ip=[] 
for i in range(len(results)):
    ip.append(results[i].text)  
    movie_reviews=movie_reviews+ip  # adding the reviews of one page to empty list which in future contains all the reviews

movie_reviews

"""# DATA PREPROCESSING AND CLEANING

# adding the reviews of one page
"""

### Removing repeated reviews 
movie_reviews = list(set(movie_reviews))

# Writing reviews into text file 
#with open("ip_snapdeal.txt","w",encoding="utf-8") as snp:
  #  snp.write(str(movie_reviews))

movie_reviews

# Joinining all the reviews into single paragraph 
ip_rev_string = " ".join(movie_reviews)

ip_rev_string

"""# Removing unwanted symbols incase if existsa"""

# Removing unwanted symbols incase if exists
ip_rev_string = re.sub("[^A-Za-z" "]+"," ",ip_rev_string).lower()
ip_rev_string = re.sub("[0-9" "]+"," ",ip_rev_string)

ip_rev_string# SEE THE DATASET TEXT

# words that contained in iphone 7 reviews
ip_reviews_words = ip_rev_string.split(" ")

stoplist = stopwords.words('english')

with open("C:/Users/ADMIN/Desktop/VT/data sets/Text Mining - NLP/stop.txt","r") as sw:
    stopwords = sw.read()

stopwords = stopwords.split("\n")

temp = ["this","is","awsome","Data","Science"]
[i for i in stopwords if i not in "is"]

ip_reviews_words = [w for w in ip_reviews_words if not w in stoplist]

"""# Joinining all the reviews into single paragraph 

"""

# Joinining all the reviews into single paragraph 
ip_rev_string = " ".join(ip_reviews_words)

"""# Tokenization
<font color='blue'><b>
Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence.</b>
</font>
"""

# importing word_tokenize from nltk
from nltk.tokenize import word_tokenize
# Passing the string text into word tokenize for breaking the sentences
token = word_tokenize(ip_rev_string)
token

# finding the frequency distinct in the tokens
# Importing FreqDist library from nltk and passing token into FreqDist
from nltk.probability import FreqDist
fdist = FreqDist(token)
fdist

# To find the frequency of top 10 words
fdist1 = fdist.most_common(10)
fdist1

# Frequency Distribution Plot
import matplotlib.pyplot as plt
fdist.plot(30,cumulative=False)
plt.show()

"""# Stemming
<b>Stemming usually refers to normalizing words into its base form or root form.
![image.png](attachment:image.png)
Here, we have words waited, waiting and waits. Here the root word is ‘wait’. There are two methods in Stemming namely, Porter Stemming (removes common morphological and inflectional endings from words) and Lancaster Stemming (a more aggressive stemming algorithm).<b>

"""

for toke in token:
  # Importing Porterstemmer from nltk library
# Checking for the word ‘giving’ 
 from nltk.stem import PorterStemmer
 pst = PorterStemmer()
for toke in token:
 pst.stem('toke')
 print(toke)

# Checking for the list of words
stm = ["waited", "waiting", "waits"]
for word in stm :
   print(word+ ":" +pst.stem(word))

"""# Lemmatization
![image.png](attachment:image.png)

<b>In simpler terms, it is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.

For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to car.
    
Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP<b>
"""

# Importing Lemmatizer library from nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer() 
 
print('rocks :', lemmatizer.lemmatize('rocks')) 
print('corpora :', lemmatizer.lemmatize('corpora'))

"""# Stop Words
<b>“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library<b>
"""

# Removing unwanted symbols incase if exists
ip_rev_string = re.sub("[^A-Za-z" "]+"," ",ip_rev_string).lower()
ip_rev_string = re.sub("[0-9" "]+"," ",ip_rev_string)

temp = ["this","is","awsome","Data","Science"]
[i for i in temp if i not in "is"]

ip_reviews_words = [w for w in ip_reviews_words if not w in stoplist]

"""# Part of Speech Tagging
![image.png](attachment:image.png)
Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context. There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.

Once we’ve identified the language of a text document, tokenized it, and broken down the sentences, it’s time to tag it.

Part of Speech tagging (or PoS tagging) is the process of determining the part of speech of every token in a document, and then tagging it as such.

For example, we use PoS tagging to figure out whether a given token represents a proper noun or a common noun, or if it’s a verb, an adjective, or something else entirely.

Part of Speech tagging may sound simple, but much like an onion, you’d be surprised at the layers involved – and they just might make you cry. At Lexalytics, due to our breadth of language coverage, we’ve had to train our systems to understand 93 unique Part of Speech tags.
"""

for toke in token:
  print(nltk.pos_tag([toke]))

"""# Named entity recognition
<b>It is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value.<b>
![image.png](attachment:image.png)
"""

#importing chunk library from nltk
from nltk import ne_chunk
tags = nltk.pos_tag(token)
chunk = ne_chunk(tags)
chunk

"""# Chunking
Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks.
![image.png](attachment:image.png)
"""

tags = nltk.pos_tag(token)
reg = 'NP:{<DT>?<JJ>*<NN>}' 
a = nltk.RegexpParser(reg)
result = a.parse(tags)
print(result)

"""# Sentimental ANALYSIS

# VISUALIZATION OF DATASET
"""

# WordCloud can be performed on the string inputs. That is the reason we have combined 
# entire reviews into single paragraph
# Simple word cloud


wordcloud_ip = WordCloud(
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(ip_rev_string)

plt.imshow(wordcloud_ip)

"""# IMPORT POSITIVE WORD FILE"""

# positive words # Choose the path for +ve words stored in system
with open("C:/Users/ADMIN/Desktop/VT/data sets/Text Mining - NLP/positive-words.txt","r") as pos:
  poswords = pos.read().split("\n")

"""# IMPORT NEGATIVE WORD FILE"""

# negative words  Choose path for -ve words stored in system
with open("C:/Users/ADMIN/Desktop/VT/data sets/Text Mining - NLP/negative-words.txt","r") as neg:
  negwords = neg.read().split("\n")

"""# VISUALLIZATION OF NEGATIVE WORD WHICH IN DATSET"""

# negative word cloud
# Choosing the only words which are present in negwords
ip_neg_in_neg = " ".join ([w for w in ip_reviews_words if w in negwords])

wordcloud_neg_in_neg = WordCloud(
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(ip_neg_in_neg)

plt.imshow(wordcloud_neg_in_neg)

"""# VISUALIZATION OF POSITIVE WORD WHICH IS IN DATASET"""

# Positive word cloud
# Choosing the only words which are present in positive words
ip_pos_in_pos = " ".join ([w for w in ip_reviews_words if w in poswords])
wordcloud_pos_in_pos = WordCloud(
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(ip_pos_in_pos)

plt.imshow(wordcloud_pos_in_pos)



